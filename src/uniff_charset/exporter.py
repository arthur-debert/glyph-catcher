"""
Module for exporting processed Unicode data to various formats.
"""

import gzip
import os
import shutil

from .config import get_output_filename
from .exporters import registry
from .processor import (
    filter_by_dataset,
    filter_by_unicode_blocks,
    load_master_data_file,
)
from .types import ExportOptions


def export_data(
    unicode_data: dict[str, dict[str, str]],
    aliases_data: dict[str, list[str]],
    options: ExportOptions,
) -> list[str]:
    """
    Export Unicode data to the specified format(s).

    Args:
        unicode_data: Dictionary mapping code points to character information
        aliases_data: Dictionary mapping code points to lists of aliases
        options: Export options

    Returns:
        List of paths to the generated output files
    """
    # If use_master_file is True and master_file_path is provided, load data from the
    # master file
    if options.use_master_file and options.master_file_path:
        try:
            loaded_unicode_data, loaded_aliases_data = load_master_data_file(
                options.master_file_path
            )

            if loaded_unicode_data and loaded_aliases_data:
                unicode_data = loaded_unicode_data
                aliases_data = loaded_aliases_data
        except Exception:
            pass

    # Filter data by dataset or Unicode blocks if specified
    if options.dataset:
        unicode_data, aliases_data = filter_by_dataset(
            unicode_data, aliases_data, options.dataset
        )
    elif options.unicode_blocks:
        unicode_data, aliases_data = filter_by_unicode_blocks(
            unicode_data, aliases_data, options.unicode_blocks
        )

    # Create output directory if it doesn't exist
    os.makedirs(options.output_dir, exist_ok=True)

    # Determine which formats to export
    formats = (
        registry.get_supported_formats()
        if options.format_type == "all"
        else [options.format_type]
    )

    # Optimized export: process all formats at once to avoid multiple reads of master data
    return optimized_export(unicode_data, aliases_data, formats, options)


def optimized_export(
    unicode_data: dict[str, dict[str, str]],
    aliases_data: dict[str, list[str]],
    formats: list[str],
    options: ExportOptions,
) -> list[str]:
    """
    Export Unicode data to multiple formats at once in an optimized way.

    Args:
        unicode_data: Dictionary mapping code points to character information
        aliases_data: Dictionary mapping code points to lists of aliases
        formats: List of formats to export to
        options: Export options

    Returns:
        List of paths to the generated output files
    """
    if not unicode_data:
        return []

    output_files = []
    file_handlers = {}
    exporters = {}
    temp_filenames = {}
    csv_writers = {}
    max_aliases_by_fmt = {}

    try:
        # 1. Initialize exporters and open files for all formats
        for fmt in formats:
            exporter = registry.get_exporter(fmt)
            if not exporter:
                continue

            exporters[fmt] = exporter

            # Get the output filename based on format and dataset
            filename = get_output_filename(fmt, options.dataset)
            output_filename = os.path.join(options.output_dir, filename)

            # Create a temporary file for uncompressed output
            temp_filename = output_filename
            if options.compress:
                temp_filename = output_filename + ".temp"

            temp_filenames[fmt] = (temp_filename, output_filename)

            # Open the file for writing
            file_handlers[fmt] = open(temp_filename, "w", encoding="utf-8")

            # Pre-calculate max aliases for CSV format
            if fmt == "csv":
                import csv

                max_aliases = 0
                if aliases_data:
                    for cp in unicode_data:
                        if cp in aliases_data:
                            max_aliases = max(max_aliases, len(aliases_data[cp]))
                max_aliases_by_fmt[fmt] = max_aliases

                # Create CSV writer
                csv_writers[fmt] = csv.writer(file_handlers[fmt])

                # Write header row
                headers = ["code_point", "character", "name", "category", "block"]
                for i in range(1, max_aliases + 1):
                    headers.append(f"alias_{i}")
                csv_writers[fmt].writerow(headers)
            elif fmt == "json":
                file_handlers[fmt].write("[\n")
                # Track if we've written the first item
                temp_filenames[fmt] = (temp_filename, output_filename, False)
            elif fmt == "lua":
                file_handlers[fmt].write("-- Auto-generated unicode data module\n")
                file_handlers[fmt].write("-- Generated by uniff-gen\n")
                file_handlers[fmt].write("return {\n")

        # 2. Process each record once and write to all formats
        for code_point_hex, data in unicode_data.items():
            current_aliases = aliases_data.get(code_point_hex, [])

            # Write this record to all formats
            for fmt in formats:
                if fmt not in file_handlers:
                    continue

                file_handler = file_handlers[fmt]

                if fmt == "csv":
                    # Write CSV row
                    row = [
                        f"U+{code_point_hex}",
                        data["char_obj"],
                        data["name"],
                        data["category"],
                        data.get("block", "Unknown Block"),
                    ]

                    max_aliases = max_aliases_by_fmt[fmt]
                    for i in range(max_aliases):
                        row.append(current_aliases[i] if i < len(current_aliases) else "")

                    csv_writers[fmt].writerow(row)

                elif fmt == "json":
                    # Add JSON entry
                    import json

                    entry = {
                        "code_point": f"U+{code_point_hex}",
                        "character": data["char_obj"],
                        "name": data["name"],
                        "category": data["category"],
                        "block": data.get("block", "Unknown Block"),
                        "aliases": current_aliases,
                    }
                    json_str = json.dumps(entry, ensure_ascii=False, indent=2)

                    # Add comma if not the first item
                    temp_filename, output_filename, has_items = temp_filenames[fmt]
                    if has_items:
                        file_handler.write(",\n")
                    else:
                        # Mark that we've written the first item
                        temp_filenames[fmt] = (temp_filename, output_filename, True)

                    file_handler.write(json_str)

                elif fmt == "lua":
                    # Handle special characters for Lua
                    char = data["char_obj"]
                    if char == "\n":
                        char = "\\n"
                    elif char == "\r":
                        char = "\\r"
                    elif char == "\t":
                        char = "\\t"
                    elif char == '"':
                        char = '\\"'
                    elif char == "\\":
                        char = "\\\\"
                    elif ord(char) < 32:  # Other control characters
                        char = f"\\{ord(char):03d}"

                    # Helper function to properly escape Lua strings
                    def escape_lua_string(s):
                        # First escape backslashes
                        s = s.replace("\\", "\\\\")
                        # Then escape other special characters
                        s = s.replace('"', '\\"')
                        s = s.replace("\n", "\\n")
                        s = s.replace("\r", "\\r")
                        s = s.replace("\t", "\\t")
                        # Replace any other control characters
                        result = ""
                        for c in s:
                            if ord(c) < 32 and c not in "\n\r\t":
                                result += f"\\{ord(c):03d}"
                            else:
                                result += c
                        return result

                    # Escape special characters in all string fields
                    name = escape_lua_string(data["name"])
                    category = escape_lua_string(data["category"])
                    block = escape_lua_string(data.get("block", "Unknown Block"))

                    file_handler.write("  {\n")
                    file_handler.write(f'    code_point = "U+{code_point_hex}",\n')
                    file_handler.write(f'    character = "{char}",\n')
                    file_handler.write(f'    name = "{name}",\n')
                    file_handler.write(f'    category = "{category}",\n')
                    file_handler.write(f'    block = "{block}",\n')

                    # Write aliases as a Lua table
                    if current_aliases:
                        file_handler.write("    aliases = {\n")
                        for alias in current_aliases:
                            # Use the same escaping function for aliases
                            escaped_alias = escape_lua_string(alias)
                            file_handler.write(f'      "{escaped_alias}",\n')
                        file_handler.write("    },\n")
                    else:
                        file_handler.write("    aliases = {},\n")

                    file_handler.write("  },\n")

                elif fmt == "txt":
                    # Format: character|name|code_point|category|block|alias1|alias2|...
                    # Optimized for grep with searchable fields first
                    line_parts = [
                        data["char_obj"],
                        data["name"],
                        f"U+{code_point_hex}",
                        data["category"],
                        data.get("block", "Unknown Block"),
                    ]

                    # Add aliases if they exist
                    if current_aliases:
                        line_parts.extend(current_aliases)

                    # Join with pipe separator
                    file_handler.write("|".join(line_parts) + "\n")

        # 3. Write footers and close files
        for fmt in formats:
            if fmt not in file_handlers:
                continue

            file_handler = file_handlers[fmt]

            # Write format-specific footers
            if fmt == "json":
                file_handler.write("\n]")
            elif fmt == "lua":
                file_handler.write("}\n")

            file_handler.close()

        # 4. Handle compression and verification
        for fmt in formats:
            if fmt not in exporters:
                continue

            if fmt == "json":
                temp_filename, output_filename, _ = temp_filenames[fmt]
            else:
                temp_filename, output_filename = temp_filenames[fmt]

            # Compress the file if requested
            if options.compress:
                compress_file(temp_filename, output_filename)
                os.remove(temp_filename)  # Remove the temporary uncompressed file
                output_filename = output_filename + ".gz"
            else:
                # Validate the exported file
                exporters[fmt].verify(temp_filename)

            output_files.append(output_filename)

        return output_files

    except Exception as e:
        print(f"Error in optimized export: {e}")
        # Clean up any open file handlers
        for file_handler in file_handlers.values():
            if not file_handler.closed:
                file_handler.close()
        return []


def compress_file(input_file: str, output_file: str) -> bool:
    """
    Compress a file using gzip.

    Args:
        input_file: Path to the file to compress
        output_file: Path to the output file

    Returns:
        True if the compression was successful, False otherwise
    """
    try:
        with open(input_file, "rb") as f_in:
            with gzip.open(output_file + ".gz", "wb", compresslevel=9) as f_out:
                shutil.copyfileobj(f_in, f_out)
        return True
    except Exception as e:
        print(f"Error compressing file: {e}")
        return False


def save_source_files(file_paths: dict[str, str], output_dir: str) -> None:
    """
    Save the source files to the output directory.

    Args:
        file_paths: Dictionary mapping file types to file paths
        output_dir: Directory to save the files to
    """
    try:
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Get XDG data directory for source files
        xdg_data_dir = os.environ.get(
            "XDG_DATA_HOME", os.path.join(os.path.expanduser("~"), ".local", "share")
        )
        source_files_dir = os.path.join(xdg_data_dir, "uniff-gen", "source-files")

        # Create XDG directory if it doesn't exist
        os.makedirs(source_files_dir, exist_ok=True)

        # Copy each source file to the XDG data directory
        for file_type, file_path in file_paths.items():
            if os.path.exists(file_path):
                # Map file types to more descriptive filenames
                if file_type == "unicode_data":
                    filename = "UnicodeData.txt"
                elif file_type == "name_aliases":
                    filename = "NameAliases.txt"
                elif file_type == "names_list":
                    filename = "NamesList.txt"
                elif file_type == "cldr_annotations":
                    filename = "en.xml"
                else:
                    filename = os.path.basename(file_path)

                dest_path = os.path.join(source_files_dir, filename)
                shutil.copy2(file_path, dest_path)
    except Exception:
        pass


def decompress_file(input_filename: str, output_filename: str) -> None:
    """
    Decompress a gzip compressed file.

    Args:
        input_filename: Path to the compressed input file
        output_filename: Path to the output decompressed file
    """
    try:
        with (
            gzip.open(input_filename, "rb") as f_in,
            open(output_filename, "wb") as f_out,
        ):
            f_out.write(f_in.read())
    except Exception:
        pass
